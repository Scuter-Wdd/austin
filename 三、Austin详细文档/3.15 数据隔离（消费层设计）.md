**视频介绍讲解**：
[![#17 SpringBoot接入Kafka.mp4 (896.4MB)](https://gw.alipayobjects.com/mdn/prod_resou/afts/img/A*NNs6TKOR3isAAAAAAAAAAABkARQnAQ)]()[![#18 使用线程池消费MQ.mp4 (324.44MB)](https://gw.alipayobjects.com/mdn/prod_resou/afts/img/A*NNs6TKOR3isAAAAAAAAAAABkARQnAQ)]()
今天要聊的是`austi-handler`模块的**消费数据隔离**，先看下**最初版**的实现方案：
![image.png](https://cdn.nlark.com/yuque/0/2022/png/1285871/1649404627928-0d7fdf15-bd18-4c45-936a-872c0c01f276.png#averageHue=%23f8f5f2&clientId=ube4518a9-0e9b-4&from=paste&height=204&id=ucfb44889&originHeight=204&originWidth=1094&originalType=binary&ratio=1&rotation=0&showTitle=false&size=93406&status=done&style=none&taskId=u24e37256-7053-476d-b8d1-481d8d76fad&title=&width=1094)

austin-api接收到了请求之后，将请求发往Kafka，topicName为austin。而在austin-handler起了一个`groupName`名为**austinGroup**监听austin这个topic的数据，进而实现消息发送。

从系统功能性来说，austin项目是可以发送多种类型消息的：短信、微信小程序、邮件等等等
![](https://cdn.nlark.com/yuque/0/2022/jpeg/1285871/1649404639103-14542858-9279-480f-9cff-1f30314ee5d7.jpeg#averageHue=%23f8edd9&clientId=ube4518a9-0e9b-4&from=paste&id=u730d989f&originHeight=908&originWidth=754&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uc63996a3-5b3b-436a-b4d9-28a14a3f5e0&title=)
那如果是单个topic单个group的话，有没有想过一个问题：**如果某个发送渠道接口存在异常，超时了，此时会怎么样**？

没错，**消息都会堵住**，因为它们消费同一个topic，用的是同一个消费者。
![](https://cdn.nlark.com/yuque/0/2022/jpeg/1285871/1649404639099-b472dc16-a842-455e-97d2-9780916b77da.jpeg#averageHue=%23f7f6f5&clientId=ube4518a9-0e9b-4&from=paste&id=ub7a4af2b&originHeight=618&originWidth=1532&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u4805bbb7-88c3-49d1-8faf-41565f2fac8&title=)
要破局？很简单。**多topic多group就行啦**。
![](https://cdn.nlark.com/yuque/0/2022/jpeg/1285871/1649404682940-cadb2bfe-2a16-47c1-98fd-7787f450c60f.jpeg#averageHue=%23edddcd&clientId=ube4518a9-0e9b-4&from=paste&id=FqCP6&originHeight=670&originWidth=1322&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ub2eac96c-00ac-46eb-9557-58233a610ed&title=)

上面这种能解决所有问题吗？**并不**。即便是同一个渠道，但不同类型的消息发送特性是不一样的。比如我要发push**营销**消息，有可能在某个时刻就要推送4000W的人群。

那这4000W人在短时间内完全发送出去，不太现实。这很可能意味着会影响到**通知类**的push消息

还要破局？很简单。 毕竟我们在设计**消息模板**的时候就已经考虑到这点了。消息模板有msgType字段来标识当前的模板属于哪种类型，**那我们可以根据不同的消息类型再划分对应的group**。
![](https://cdn.nlark.com/yuque/0/2022/jpeg/1285871/1649404703390-2866013c-ecdf-4d0c-ab76-d98eb7b79357.jpeg#averageHue=%23f8f5f3&clientId=ube4518a9-0e9b-4&from=paste&id=u18126344&originHeight=836&originWidth=1314&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u7de4aba7-1c9e-43c8-86e5-5381feea9b5&title=)
从理论上来说，我们可以为**每种渠道的每种消息类型单独区分一个topic和group**。因为topic间的数据是隔离的，不同的group间消费也是隔离的，那我们消费时肯定是数据隔离的。

不过，我目前的做法是：**单topic多group**。

**消费是隔离的，但生产的topic是共享的**，我认为这样代码会更加清晰和易懂些，**后期如果存在瓶颈了我们可以继续改**。
![](https://cdn.nlark.com/yuque/0/2022/jpeg/1285871/1649404758234-375f57f4-6693-4911-ae68-5989ae2d3ce5.jpeg#averageHue=%23f0ecdc&clientId=ube4518a9-0e9b-4&from=paste&id=u667431e2&originHeight=978&originWidth=1606&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u758c9228-019b-413d-a813-3ca7171aa40&title=)
已经定了通过单topic多group来实现数据隔离。比如，我目前定义了**6个渠道**(im/push/邮件/短信/小程序/微信服务号)和3种消息类型(通知/营销/验证码)，那相当于起了**18**个消费者。

从kafka获取得到消息以后，我**暂定**规划是走几个步骤：**消息丢弃->夜间屏蔽**->**去重**->**真正发送**

![image.png](https://cdn.nlark.com/yuque/0/2022/png/1285871/1649407520651-baa7015e-5b80-4cc6-9776-13840cb8d066.png#averageHue=%23cfd7c5&clientId=ube4518a9-0e9b-4&from=paste&height=213&id=udb22a52c&originHeight=213&originWidth=1098&originalType=binary&ratio=1&rotation=0&showTitle=false&size=82135&status=done&style=none&taskId=ueaf53a41-09e2-411d-a84b-166ffb0b2bc&title=&width=1098)

从本质上看**去重**和**发送消息**都是**网络IO密集型**。于是，为了**提高吞吐量**，我这边决定消费Kafka后存入缓存，**做一层缓冲区**。

![image.png](https://cdn.nlark.com/yuque/0/2022/png/1285871/1649407535174-69edf65a-620f-4127-88cf-9dd6920c0a80.png#averageHue=%23e7d9c5&clientId=ube4518a9-0e9b-4&from=paste&height=144&id=u250cd4d7&originHeight=144&originWidth=1165&originalType=binary&ratio=1&rotation=0&showTitle=false&size=70557&status=done&style=none&taskId=u3299477c-c5ed-4d21-9e7d-764bc5b74eb&title=&width=1165)

做一层缓冲区可提高吞吐量，但同样会带来别的问题。如：当应用重启时，缓冲区的数据还没消费完，那是不是就会丢失？

缓冲区给我的第一反应是实现**生产者消费者模式**

![image.png](https://cdn.nlark.com/yuque/0/2022/png/1285871/1649407551212-fef6ed2e-c309-472d-9ec3-bcaca93e680e.png#averageHue=%23f4efef&clientId=ube4518a9-0e9b-4&from=paste&height=188&id=u13dc6ea0&originHeight=188&originWidth=1091&originalType=binary&ratio=1&rotation=0&showTitle=false&size=76223&status=done&style=none&taskId=u429f2d8d-f0f2-4dae-958c-90ae0167d6b&title=&width=1091)

要实现这种模式，我初想了下挺简单的：消费Kafka的消息作为生产者，然后把数据扔进阻塞队列上，开多个线程去消费阻塞队列的数据就完事了。

后来又想了下，**直接线程池不就完事了吗？线程池不就是生产者和消费者的实现吗**。

于是乎，架构就变成了下图：
![image.png](https://cdn.nlark.com/yuque/0/2022/png/1285871/1649407570764-f7b78fd5-18d0-4819-a9f7-2f1f6cc65dcd.png#averageHue=%23dfe1d0&clientId=ube4518a9-0e9b-4&from=paste&height=581&id=ud2fea36f&originHeight=581&originWidth=1140&originalType=binary&ratio=1&rotation=0&showTitle=false&size=252448&status=done&style=none&taskId=u1bc6ae65-f571-49f8-a4c8-8377100545a&title=&width=1140)
